{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "def md(s):\n",
    "    display(Markdown(s))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "API_URL = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"You are a helpful AI assistant that can call functions when appropriate. If the question asked does not relate to the functions, do not respond in a JSON format. Respond in JSON format with the following structure:\n",
    "\n",
    "If a function call is needed:\n",
    "{\n",
    "  \"action\": \"call_function\",\n",
    "  \"function\": \"<function_name>\",\n",
    "  \"arguments\": {\"arg1\": value1, \"arg2\": value2, ...}\n",
    "}\n",
    "\n",
    "Available functions:\n",
    "- toothnumbers: Calculate gear diameters. Arguments: m (module in mm, float), N1 (teeth on driving gear, int), gearratio (target gear ratio, float).\n",
    "- Add: Add two numbers. Arguments: x (float), y (float).\n",
    "- Square: Square a number. Arguments: x (float).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def getinput(prompt):\n",
    "    try:\n",
    "        return input(prompt)\n",
    "    except EOFError:\n",
    "        return None\n",
    "    except KeyboardInterrupt:  # When ESC is pressed\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_conversation_history(messages):\n",
    "    \"\"\"\n",
    "    Displays conversation history in a two-column markdown table in a Jupyter notebook.\n",
    "    \n",
    "    Args:\n",
    "        messages: List of strings alternating between user queries and LLM responses\n",
    "    \"\"\"\n",
    "    from IPython.display import clear_output\n",
    "    # Clear the output to avoid clutter\n",
    "    clear_output(wait=True)\n",
    "    # Create table header\n",
    "    table = \"| User Query | LLM Response |\\n\"\n",
    "    table += \"|------------|-------------|\\n\"\n",
    "    \n",
    "    # Add rows to the table, processing messages in pairs\n",
    "    for i in range(0, len(messages), 2):\n",
    "        user_msg = messages[i]\n",
    "        \n",
    "        # Check if there's a corresponding LLM response\n",
    "        llm_msg = messages[i+1] if i+1 < len(messages) else \"\"\n",
    "        \n",
    "        # Add row to table, escaping any pipe characters in the messages\n",
    "        user_msg_escaped = user_msg.replace(\"|\", \"\\\\|\")\n",
    "        llm_msg_escaped = llm_msg.replace(\"|\", \"\\\\|\")\n",
    "        \n",
    "        table += f\"| {user_msg_escaped} | {llm_msg_escaped} |\\n\"\n",
    "    \n",
    "    # Display the table using the md() function\n",
    "    md(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def toothnumbers(m, N1, gearratio):\n",
    "    \"\"\"\n",
    "    Calculate the diameters of the gears in a gear train.\n",
    "\n",
    "    Parameters:\n",
    "    m (float): The module of the gears (mm)\n",
    "    N1 (int): The number of teeth on the driving gear.\n",
    "    gearratio (float): The target gear ratio.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing gear details and the actual gear ratio.\n",
    "    \"\"\"\n",
    "    d1 = m * N1  # Diameter of the driving gear\n",
    "    N2 = int(N1 * gearratio)  # Number of teeth on the driven gear\n",
    "    d2 = m * N2  # Diameter of the driven gear\n",
    "    actual_gear_ratio = N2 / N1  # Actual gear ratio\n",
    "\n",
    "    if abs(actual_gear_ratio - gearratio) > 0.01:\n",
    "        raise ValueError(f\"Gear ratio mismatch: {actual_gear_ratio:.2f} != {gearratio:.2f}\")\n",
    "\n",
    "    return {\n",
    "        \"text\": \"Computation successful\",\n",
    "        \"number_of_teeth_gear1\": N1,\n",
    "        \"diameter_gear1_mm\": d1,\n",
    "        \"number_of_teeth_gear2\": N2,\n",
    "        \"diameter_gear2_mm\": d2,\n",
    "        \"actual_gear_ratio\": actual_gear_ratio\n",
    "    }\n",
    "\n",
    "def add(x, y):\n",
    "    addition = x+y\n",
    "    return {\n",
    "        \"Addition result\": addition\n",
    "    }\n",
    "\n",
    "def square(x):\n",
    "    square = x**2\n",
    "    return {\n",
    "        \"Square result\": square\n",
    "    }\n",
    "\n",
    "\n",
    "functions = {\n",
    "    \"toothnumbers\": toothnumbers,\n",
    "    \"Add\": add,\n",
    "    \"Square\": square\n",
    "    # Add more functions here as needed, e.g., \"another_function\": another_function\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def generate_response(prompt):\n",
    "    full_prompt = f\"{SYSTEM_PROMPT}\\n\\nUser: {prompt}\\nAssistant:\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            API_URL,\n",
    "            json={\n",
    "                \"model\": \"llama3.2:3b\",\n",
    "                \"prompt\": full_prompt,\n",
    "                \"stream\": False\n",
    "            },\n",
    "            timeout=60  # Increased timeout to 60 seconds\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        result = response.json()\n",
    "        return result.get(\"response\", \"No response\")\n",
    "    except requests.Timeout:\n",
    "        return \"Error: Request timed out. The model is taking too long to respond. Please try again.\"\n",
    "    except requests.ConnectionError:\n",
    "        return \"Error: Could not connect to the Ollama server. Please ensure it is running.\"\n",
    "    except requests.RequestException as e:\n",
    "        return f\"Error connecting to Llama model: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "| User Query | LLM Response |\n",
       "|------------|-------------|\n",
       "| add 2 and 3 | {'Addition result': 5} |\n",
       "| square 3456 | {'Square result': 11943936} |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "\n",
    "def extract_json(response_text):\n",
    "    \"\"\" Checks if the response by the model is in JSON format, and returns the response\"\"\"\n",
    "    start = response_text.find('{')\n",
    "    end = response_text.rfind('}') + 1\n",
    "    if start != -1 and end != -1:\n",
    "        json_str = response_text[start:end]\n",
    "        try:\n",
    "            return json.loads(json_str)\n",
    "        except json.JSONDecodeError:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "CHAT_HISTORY = []\n",
    "\n",
    "while True:\n",
    "    user_input = getinput(\"Enter a prompt (or 'q' to quit): \")\n",
    "    if user_input in (None, \"q\", \"Q\"):\n",
    "        break\n",
    "\n",
    "    CHAT_HISTORY.append(user_input)\n",
    "    response = generate_response(user_input)\n",
    "\n",
    "    json_response = extract_json(response) # checks if the llm responds with a json as expected\n",
    "    if json_response:\n",
    "        action = json_response.get(\"action\")\n",
    "        if action == \"call_function\":\n",
    "            function_name = json_response.get(\"function\")\n",
    "            arguments = json_response.get(\"arguments\", {})\n",
    "            if function_name in functions:\n",
    "                try:\n",
    "                    result = functions[function_name](**arguments)\n",
    "                    if function_name == \"toothnumbers\":\n",
    "                        final_response = (\n",
    "                            f\"{result['text']}: Driver Gear: {result['number_of_teeth_gear1']} teeth, \"\n",
    "                            f\"{result['diameter_gear1_mm']:.1f}mm diameter, Driven Gear: \"\n",
    "                            f\"{result['number_of_teeth_gear2']} teeth, {result['diameter_gear2_mm']:.1f}mm diameter, \"\n",
    "                            f\"Actual Gear Ratio: {result['actual_gear_ratio']:.3f}\"\n",
    "                        )\n",
    "                    elif function_name == \"add\":\n",
    "                        final_response = f\"{result['add']}\"\n",
    "                    elif function_name == \"square\":\n",
    "                        final_response = f\"{result['square']}\"\n",
    "                    else:\n",
    "                        final_response = str(result)  # Default formatting for other functions\n",
    "                except Exception as e:\n",
    "                    final_response = f\"Error in function call: {str(e)}\"\n",
    "            else:\n",
    "                final_response = f\"Unknown function: {function_name}\"\n",
    "        elif action == \"respond\":\n",
    "            final_response = json_response.get(\"text\", \"\")\n",
    "        else:\n",
    "            final_response = \"Invalid action in response.\"\n",
    "    else:\n",
    "        final_response = f\"Could not parse JSON from response. Raw response: {response}\"\n",
    "\n",
    "    CHAT_HISTORY.append(final_response)\n",
    "    display_conversation_history(CHAT_HISTORY)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
